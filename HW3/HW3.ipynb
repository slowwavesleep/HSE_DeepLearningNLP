{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Что нужно было сделать\n",
    "1. Написать Dataset для задачи seq2seq\n",
    "2. Реализовать модель\n",
    "3. Сделать цикл обучения\n",
    "4. Реализовать метод генерации ответа по вопросу с помощью вашей модели\n",
    "\n",
    "## Что было сделано (отмечено ✓)\n",
    "\n",
    "1. Сделать модель, основанную на lstm/gru 5 баллов ✓\n",
    "2. Сделать модель, основанную на cnn 7 баллов\n",
    "3. Сделать модель, основанную на трансформере (реализовать все слои самому) 10 баллов\n",
    "4. Добавить в rnn/cnn модель attention 5 баллов ✓\n",
    "5. Реализовать жадное семплирование (генерацию по самому вероятному токену, как выше в языковой модели) 3 балла ✓\n",
    "6. Реализовать beam search 5 баллов\n",
    "7. Реализовать nucleus sampling 5 баллов ✓\n",
    "8. Добавить condition в модель 3 балла\n",
    "9. Добавить layer norm/residual в cnn или rnn модель 1 балл ✓\n",
    "10. Реализовать аккамуляцию градиентов 1 балл\n",
    "11. Сделать телеграм бота 2 балла\n",
    "Конвертируем json в более удобный формат.\n",
    "\n",
    "## Шаги решения\n",
    "\n",
    "`1.` Сначала я преобразовал данные в более удобный формат: файлы `source` и\n",
    "`target` содержат пары вопросов и ответов на них.\n",
    "```\n",
    "python convert_data.py\n",
    "```\n",
    "`2.` Затем данные были разделены на train, dev, test.\n",
    "```\n",
    "python split_data.py data/source data/target data 42\n",
    "```\n",
    "`3.` Обучил модель и нагенерировал ответы с помощью **greedy search**\n",
    "и **nucleus sampling** (файлы `greedy_responses.txt` и `nucleus_responses.txt` в\n",
    "папке `results`).\n",
    "```\n",
    "python train.py\n",
    "```\n",
    "\n",
    "Ниже процесс обучения для наглядности."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to create temporary data...\n",
      "Temporary data successfully created!\n",
      "BPE model successfully trained!\n",
      "Attempting to remove temporary data...\n",
      "Temporary data successfully removed!\n",
      "Using GPU...\n",
      "Epoch: 1\n",
      "Train: loss - 6.6433 | perplexity - 767.626\n",
      "Validation: loss - 5.9259 | perplexity - 374.604\n",
      "Epoch: 2\n",
      "Train: loss - 5.8663 | perplexity - 352.942\n",
      "Validation: loss - 5.6647 | perplexity - 288.488\n",
      "Epoch: 3\n",
      "Train: loss - 5.6290 | perplexity - 278.373\n",
      "Validation: loss - 5.6384 | perplexity - 281.014\n",
      "Epoch: 4\n",
      "Train: loss - 5.4651 | perplexity - 236.294\n",
      "Validation: loss - 5.4313 | perplexity - 228.448\n",
      "Epoch: 5\n",
      "Train: loss - 5.3401 | perplexity - 208.530\n",
      "Validation: loss - 5.3402 | perplexity - 208.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 691/691 [03:57<00:00,  2.91it/s, loss=6.32, perplexity=557]    \n",
      "Evaluate: 100%|██████████| 82/82 [00:09<00:00,  8.33it/s, loss=5.93, perplexity=375]\n",
      "Train: 100%|██████████| 691/691 [03:58<00:00,  2.90it/s, loss=5.83, perplexity=340]\n",
      "Evaluate: 100%|██████████| 82/82 [00:10<00:00,  8.16it/s, loss=5.66, perplexity=288]\n",
      "Train: 100%|██████████| 691/691 [04:00<00:00,  2.87it/s, loss=5.61, perplexity=272]\n",
      "Evaluate: 100%|██████████| 82/82 [00:09<00:00,  8.23it/s, loss=5.64, perplexity=281]\n",
      "Train: 100%|██████████| 691/691 [04:00<00:00,  2.88it/s, loss=5.45, perplexity=233]\n",
      "Evaluate: 100%|██████████| 82/82 [00:09<00:00,  8.28it/s, loss=5.43, perplexity=228]\n",
      "Train: 100%|██████████| 691/691 [04:00<00:00,  2.87it/s, loss=5.33, perplexity=207]\n",
      "Evaluate: 100%|██████████| 82/82 [00:09<00:00,  8.26it/s, loss=5.34, perplexity=209]\n",
      "Generating responses using greedy search...: 100%|██████████| 100/100 [00:03<00:00, 32.92it/s]\n",
      "Generating responses using nucleus search...: 100%|██████████| 100/100 [00:09<00:00, 10.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.data import basic_load\n",
    "from src.tokenization import train_bpe, batch_tokenize\n",
    "from src.datasets import QAData\n",
    "from src.models import MyNet\n",
    "from src.training import training_cycle\n",
    "from src.generation import Generator\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import youtokentome as yttm\n",
    "\n",
    "\n",
    "SOURCE_TRAIN_PATH = 'data/source.train'\n",
    "SOURCE_DEV_PATH = 'data/source.dev'\n",
    "SOURCE_TEST_PATH = 'data/source.test'\n",
    "TARGET_TRAIN_PATH = 'data/target.train'\n",
    "TARGET_DEV_PATH = 'data/target.dev'\n",
    "TARGET_TEST_PATH = 'data/target.test'\n",
    "BPE_TEXT_PATH = 'models/bpe_raw.txt'\n",
    "BPE_MODEL_PATH = 'models/bpe_qa.model'\n",
    "RESPONSES_GREEDY_PATH = 'results/greedy_responses.txt'\n",
    "RESPONSES_NUCLEUS_PATH = 'results/nucleus_responses.txt'\n",
    "VOCAB_SIZE = 7000\n",
    "MAX_SOURCE_LEN = 40\n",
    "MAX_TARGET_LEN = 40\n",
    "PAD_INDEX = 0\n",
    "UNK_INDEX = 1\n",
    "UNK_INDEX = 2\n",
    "EOS_INDEX = 3\n",
    "BATCH_SIZE = 512\n",
    "EMB_DIM = 512\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "\n",
    "TRAIN_BPE = True\n",
    "TRAIN_NET = True\n",
    "GENERATE = True\n",
    "\n",
    "source_train = basic_load(SOURCE_TRAIN_PATH)\n",
    "target_train = basic_load(TARGET_TRAIN_PATH)\n",
    "source_dev = basic_load(SOURCE_DEV_PATH)\n",
    "target_dev = basic_load(TARGET_DEV_PATH)\n",
    "\n",
    "assert len(source_train) == len(target_train)\n",
    "assert len(source_dev) == len(target_dev)\n",
    "\n",
    "if TRAIN_BPE:\n",
    "\n",
    "    train_bpe(sentences=source_train,\n",
    "              bpe_text_path=BPE_TEXT_PATH,\n",
    "              bpe_model_path=BPE_MODEL_PATH,\n",
    "              vocab_size=VOCAB_SIZE)\n",
    "\n",
    "bpe = yttm.BPE(model=BPE_MODEL_PATH)\n",
    "\n",
    "source_train_tokenized = batch_tokenize(source_train, bpe, bos=False, eos=False)\n",
    "source_dev_tokenized = batch_tokenize(source_dev, bpe, bos=False, eos=False)\n",
    "target_train_tokenized = batch_tokenize(target_train, bpe, bos=False, eos=False)\n",
    "target_dev_tokenized = batch_tokenize(target_dev, bpe, bos=False, eos=False)\n",
    "\n",
    "assert len(source_train_tokenized) == len(target_train_tokenized)\n",
    "assert len(source_dev_tokenized) == len(target_dev_tokenized)\n",
    "\n",
    "train_ds = QAData(source_train_tokenized,\n",
    "                  target_train_tokenized,\n",
    "                  MAX_SOURCE_LEN,\n",
    "                  MAX_TARGET_LEN)\n",
    "\n",
    "valid_ds = QAData(source_dev_tokenized,\n",
    "                  target_dev_tokenized,\n",
    "                  MAX_SOURCE_LEN,\n",
    "                  MAX_TARGET_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, BATCH_SIZE)\n",
    "\n",
    "GPU = torch.cuda.is_available()\n",
    "\n",
    "if GPU:\n",
    "    print('Using GPU...')\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "model = MyNet(emb_dim=EMB_DIM,\n",
    "              hidden_size=HIDDEN_SIZE,\n",
    "              vocab_size=VOCAB_SIZE,\n",
    "              dropout=0.4,\n",
    "              weight_tying=True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_INDEX)\n",
    "optimizer = torch.optim.Adam(params=model.parameters())\n",
    "\n",
    "if TRAIN_NET:\n",
    "\n",
    "    training_cycle(model, train_loader, valid_loader, optimizer, criterion,\n",
    "                   device,  3., 5)\n",
    "\n",
    "\n",
    "if GENERATE:\n",
    "\n",
    "    source_test = basic_load(SOURCE_TEST_PATH)[:100]\n",
    "    target_test = basic_load(TARGET_TEST_PATH)[:100]\n",
    "\n",
    "    model.load_state_dict(torch.load('models/best_language_model_state_dict.pth'))\n",
    "\n",
    "    generator = Generator(bpe, model, device)\n",
    "    generator.to_file(source_test, target_test, RESPONSES_GREEDY_PATH, 'greedy')\n",
    "    generator.to_file(source_test, target_test, RESPONSES_NUCLEUS_PATH, 'nucleus')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Greedy Search\n",
    "\n",
    "В целом, выбирая наиболее вероятный токен, можно получить какой-то относительно вменяемый\n",
    "(с учетом источника) и может быть даже реалистичный ответ. Ниже привел несколько вопросов\n",
    "и ответов модели на них.\n",
    "\n",
    "```\n",
    "Q: счастье - что ето\n",
    "\n",
    "A: счастье это счастье!\n",
    "```\n",
    "\n",
    "```\n",
    "Q: вопрос знатокам музыки.\n",
    "\n",
    "A: [ссылка заблокирована по решению администрации проекта]\n",
    "```\n",
    "\n",
    "```\n",
    "Q: сколько дней я работаю на вазе?\n",
    "\n",
    "A: 15 минут 20 минут 10 дней\n",
    "```\n",
    "\n",
    "```\n",
    "Q: может надо было?\n",
    "\n",
    "A: может быть и не надо.\n",
    "```\n",
    "\n",
    "```\n",
    "Q: посоветуйте ужасы на подобии у холмов есть глаза и зеркала\n",
    "\n",
    "A: у холла в глазах и глазах\n",
    "```\n",
    "#### Nucleus Sampling\n",
    "\n",
    "Случайный выбор из подвыборки токенов, суммарная вероятность которых больше `p`, к сожалению\n",
    "даёт не очень хорошие результаты. Полагаю, что использовать nucleus sampling имеет смысл\n",
    "вместе с другими методами.\n",
    "\n",
    "Ниже приведены примеры ответов модели.\n",
    "```\n",
    "Q: если официант в ресторане что-то не то вам принёс или сделал что вы будете делать???\n",
    "\n",
    "A:  сколько снятно понимать, сказать тяжелее делают сказал плохо делать? если достаточно\n",
    "    определяет похвалиных образом насньняще привычки, предотщетрена\n",
    "```\n",
    "```\n",
    "Q: счастье - что ето\n",
    "\n",
    "A: нет жаром которые трядонемые (())))\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "Q: какие чулки мне одеть, чтоб теплые тока были? подскажите а?\n",
    "\n",
    "A:  японская керора. дорогих ходы общных тупых моденной востоки\n",
    "    главное мешком можете макси stsel nad hiki?\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "Q: а вы злые или добрые с утра, м?\n",
    "\n",
    "A:  тут породы вам?), подобный телефон. точно социальнощей работодателяуру,\n",
    "    зараженую помочь обществоку))\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}